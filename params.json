{"name":"Openstack-automation","tagline":"Openstack deployment using saltstack","body":"openstack-automation\r\n====================\r\n\r\nOpenstack deployment using saltstack\r\n\r\nThere are several methods for automated deployment of openstack cluster. In this blog we attempt to do it using [saltstack](http://docs.saltstack.com/ \"Saltstack\"). There is almost no coding involved and it can be easily maintained. Above all it is as easy as talking to your servers and asking them to configure themselves. \r\n\r\nSaltstack provides us an infrastructure management framework that makes our job easier. Saltstack supports most of tasks that you would want to perform while installing openstack and more. We might not need any programming to do so. All we do need to do is define our cluster as below.\r\n\r\n<pre>\r\n{\r\n    \"controller\": [\r\n        \"hawk\"\r\n    ],\r\n    \"compute\": [\r\n        \"lammer\"\r\n    ],\r\n    \"install\": {\r\n        \"controller\": [\r\n            \"generics.host\",\r\n            \"generics.havana\",\r\n            \"generics.ntp\",\r\n            \"mysql\",\r\n            \"queue.rabbit\",\r\n            \"keystone\",\r\n            \"glance\",\r\n            \"nova\",\r\n            \"mysql.client\",\r\n            \"horizon\",\r\n            \"neutron\",\r\n            \"neutron.openvswitch\"\r\n        ],\r\n        \"compute\": [\r\n            \"generics.host\",\r\n            \"generics.havana\",\r\n            \"generics.ntp\",\r\n            \"mysql.client\",\r\n            \"nova.compute_kvm\",\r\n            \"neutron.openvswitch\"\r\n        ]\r\n    },\r\n    \"salt-master\": \"<salt master ip address here>\",\r\n    \"cluster_type\": \"openstack\",\r\n    \"config-folder\": \"cluster1\",\r\n    \"keystone.token\": \"24811ee3f7f09915bef0\",\r\n    \"keystone.endpoint\": \"http://hawk:35357/v2.0\",\r\n    \"keystone\": {\r\n        \"token\": \"24811ee3f7f09915bef0\",\r\n        \"endpoint\": \"http://hawk:35357/v2.0\",\r\n        \"dbname\": \"keystone\",\r\n        \"tenants\": {\r\n            \"admin\": {\r\n                \"users\": {\r\n                    \"admin\": {\r\n                        \"password\": \"admin_pass\",\r\n                        \"email\": \"salt@csscorp.com\",\r\n                        \"role\": \"admin\"\r\n                    }\r\n                }\r\n            },\r\n            \"service\": {\r\n                \"users\": {\r\n                    \"glance\": {\r\n                        \"password\": \"glance_pass\",\r\n                        \"email\": \"salt@csscorp.com\",\r\n                        \"role\": \"admin\"\r\n                    },\r\n                    \"nova\": {\r\n                        \"password\": \"nova_pass\",\r\n                        \"email\": \"salt@csscorp.com\",\r\n                        \"role\": \"admin\"\r\n                    },\r\n                    \"cinder\": {\r\n                        \"password\": \"cinder_pass\",\r\n                        \"email\": \"salt@csscorp.com\",\r\n                        \"role\": \"admin\"\r\n                    },\r\n                    \"neutron\": {\r\n                        \"password\": \"neutron_pass\",\r\n                        \"email\": \"salt@csscorp.com\",\r\n                        \"role\": \"admin\"\r\n                    }\r\n                }\r\n            }\r\n        },\r\n        \"roles\": [\r\n            \"admin\",\r\n            \"Member\"\r\n        ],\r\n        \"services\": {\r\n            \"keystone\": {\r\n                \"service_type\": \"identity\",\r\n                \"description\": \"keystone identity service\",\r\n                \"endpoint\": {\r\n                    \"publicurl\": \"http://{{ grains['id'] }}:5000/v2.0\",\r\n                    \"internalurl\": \"http://{{ grains['id'] }}:5000/v2.0\",\r\n                    \"adminurl\": \"http://{{ grains['id'] }}:35357/v2.0\"\r\n                }\r\n            },\r\n            \"glance\": {\r\n                \"service_type\": \"image\",\r\n                \"description\": \"glance image service\",\r\n                \"endpoint\": {\r\n                    \"publicurl\": \"http://{{ grains['id'] }}:9292\",\r\n                    \"internalurl\": \"http://{{ grains['id'] }}:9292\",\r\n                    \"adminurl\": \"http://{{ grains['id'] }}:9292\"\r\n                }\r\n            },\r\n            \"nova\": {\r\n                \"service_type\": \"compute\",\r\n                \"description\": \"nova compute service\",\r\n                \"endpoint\": {\r\n                    \"publicurl\": \"http://{{ grains['id'] }}:8774/v2/%(tenant_id)s\",\r\n                    \"internalurl\": \"http://{{ grains['id'] }}:8774/v2/%(tenant_id)s\",\r\n                    \"adminurl\": \"http://{{ grains['id'] }}:8774/v2/%(tenant_id)s\"\r\n                }\r\n            },\r\n            \"cinder\": {\r\n                \"service_type\": \"volume\",\r\n                \"description\": \"cinder volume service\",\r\n                \"endpoint\": {\r\n                    \"publicurl\": \"http://{{ grains['id'] }}:8776/v1/%(tenant_id)s\",\r\n                    \"internalurl\": \"http://{{ grains['id'] }}:8776/v1/%(tenant_id)s\",\r\n                    \"adminurl\": \"http://{{ grains['id'] }}:8776/v1/%(tenant_id)s\"\r\n                }\r\n            },\r\n            \"neutron\": {\r\n                \"service_type\": \"network\",\r\n                \"description\": \"Openstack network service\",\r\n                \"endpoint\": {\r\n                    \"publicurl\": \"http://{{ grains['id'] }}:9696\",\r\n                    \"internalurl\": \"http://{{ grains['id'] }}:9696\",\r\n                    \"adminurl\": \"http://{{ grains['id'] }}:9696\"\r\n                }\r\n            }\r\n        }\r\n    },\r\n    \"mysql\": {\r\n        \"keystone\": {\r\n            \"username\": \"keystone\",\r\n            \"password\": \"keystone_pass\",\r\n            \"sync\": \"keystone-manage db_sync\",\r\n            \"service\": \"keystone\"\r\n        },\r\n        \"glance\": {\r\n            \"username\": \"glance\",\r\n            \"password\": \"glance_pass\",\r\n            \"sync\": \"glance-manage db_sync\",\r\n            \"service\": \"glance\"\r\n        },\r\n        \"cinder\": {\r\n            \"username\": \"cinder\",\r\n            \"password\": \"cinder_pass\"\r\n        },\r\n        \"nova\": {\r\n            \"username\": \"nova\",\r\n            \"password\": \"nova_pass\",\r\n            \"sync\": \"nova-manage db sync\",\r\n            \"service\": \"nova-api\"\r\n        },\r\n        \"neutron\": {\r\n            \"username\": \"neutron\",\r\n            \"password\": \"neutron_pass\"\r\n        },\r\n        \"dash\": {\r\n            \"username\": \"dash\",\r\n            \"password\": \"dash_pass\"\r\n        }\r\n    },\r\n    \"DM\": {\r\n        \"cpu\": {\r\n            \"threshold\": 70\r\n        },\r\n        \"memory\": {\r\n            \"threshold\": 70\r\n        },\r\n        \"swap\": {\r\n            \"threshold\": 70\r\n        }\r\n    },\r\n    \"hosts\": {\r\n        \"hawk\": \"<hawk ip address here>\",\r\n        \"lammer\": \"<lammer ip address here>\"\r\n    }\r\n}\r\n</pre>\r\n\r\nWhat we saw above is the [pillar](http://docs.saltstack.com/topics/pillar/ \"Salt Pillar\") definition. Should you need to change your cluster definition you do so by changing the pillar and synchronising the changes. Although this file defines your cluster entirely the file in itself can do nothing. The entire project has been checked in [here](https://github.com/Akilesh1597/openstack-automation/ \"Openstack-Automation\").\r\n\r\nTo test it create a new [environment](http://docs.saltstack.com/ref/file_server/index.html#environments \"Salt Environments\") in your salt master configuration file and point it to where you have downloaded the project like so.\r\n\r\n<pre>\r\n\r\nfile_roots: \r\n  base: \r\n    - /srv/salt/ \r\n  havana: \r\n    - (project-directory)/file \r\npillar_roots: \r\n  base: \r\n    - /srv/pillar \r\n  havana: \r\n    - (project-directory)/pillar \r\n\r\n</pre>\r\n\r\nThis will add the havana environment to your saltstack. The 'file_roots' will have [state definitions](http://docs.saltstack.com/topics/tutorials/starting_states.html \"Salt States\") in a bunch of '.sls' files and the few special directories, while the 'pillar_roots' has your cluster definition. \r\n\r\nAt this stage I assume that you have two machines 'hawk' and 'lammer' (these are their hostnames as well as their minion id). The pillar definition file shown at the begining has a key 'cluster_type'. We will use this key to deliver commands to the two machines alone like so.\r\n\r\n<pre>\r\nsalt -C 'I@cluster_type:openstack' state.highstate\r\n</pre>\r\n\r\n\r\nThis instructs all the minions those who have 'cluster_type=openstack' defined in their pillar data to download all the state defined for them and execute the same. Once the command is run once you will note that several states have failed. Not to worry. Run the command again. You will note that each time you run will bring your cluster closer to completion. At third run you will have all your states applied successfully. At this stage you can login to your newly installed openstack setup 'http://hawk/horizon'.\r\n\r\nThe states have done almost all the stuff for you. You still have to follow [neutron deployment use cases](http://docs.openstack.org/trunk/install-guide/install/apt/content/neutron-deploy-use-cases.html \"Networking Options\") to have your network up and running. This involves creation of your intergration bridge, external bridge, physical networks etc.\r\n\r\nNote\r\n====\r\nThe state generics.havana installs a file at /etc/apt/apt.conf.d/01proxy. This points to your salt master as [apt cache proxy](https://help.ubuntu.com/community/Apt-Cacher-Server \"Apt Cache\"). If you do not want this to happen comment the contents of 'file_root/config/cluster1/common/etc/apt/apt.conf.d/01proxy'\r\n\r\n<pre>\r\nsed -i \"s/^Acquire/#Acquire/\" /etc/apt/apt.conf.d/01proxy\r\n</pre>\r\n\r\nCluster Definition\r\n==================\r\n\r\nTo make things clear lets have a look at a part of the pillar configuration.\r\n\r\n<pre>\r\n    \"controller\": [\r\n        \"hawk\"\r\n    ],\r\n    \"compute\": [\r\n        \"lammer\"\r\n    ],\r\n    \"install\": {\r\n        \"controller\": [\r\n            \"generics.host\",\r\n            \"generics.havana\",\r\n            \"generics.ntp\",\r\n            \"mysql\",\r\n            \"queue.rabbit\",\r\n            \"keystone\",\r\n            \"glance\",\r\n            \"nova\",\r\n            \"mysql.client\",\r\n            \"horizon\",\r\n            \"neutron\",\r\n            \"neutron.openvswitch\"\r\n        ],\r\n        \"compute\": [\r\n            \"generics.host\",\r\n            \"generics.havana\",\r\n            \"generics.ntp\",\r\n            \"mysql.client\",\r\n            \"nova.compute_kvm\",\r\n            \"neutron.openvswitch\"\r\n        ]\r\n    },\r\n    \"config-folder\": \"cluster1\",\r\n</pre>\r\n\r\nFirst define one controller node, which is 'hawk', and one compute node 'lammer'. \r\n\r\nUnder the “install” section we define what states to apply on a machine in order to deploy a controller or a compute node.\r\n\r\nThe “config-folder” options tells the minions where to get their configuration files from. Setting it to 'cluster1' will tell the minions to get their configuration files from 'file_roots/config/cluster1/'. These have been used extensively in state definitions.\r\n\r\nThe rest of the pillar definition is self explanatory.\r\n\r\n\r\nAdding a new cluster entity\r\n===========================\r\nLets say you need a network node all you have to do is redefine the cluster and sync.\r\n\r\nTo redefine first we need to add a new entity under \"install\" and then add machines under that entity like so\r\n\r\n<pre>\r\n    \"controller\": [\r\n        \"hawk\"\r\n    ],\r\n    \"compute\": [\r\n        \"lammer\"\r\n    ],\r\n    \"network\": [\r\n        \"goshawk\"\r\n    ],\r\n    \"install\": {\r\n        \"controller\": [\r\n            \"generics.host\",\r\n            \"generics.havana\",\r\n            \"generics.ntp\",\r\n            \"mysql\",\r\n            \"queue.rabbit\",\r\n            \"keystone\",\r\n            \"glance\",\r\n            \"nova\",\r\n            \"mysql.client\",\r\n            \"horizon\"\r\n        ],\r\n        \"network\": [\r\n            \"neutron\",\r\n            \"neutron.openvswitch\"\r\n        ],\r\n        \"compute\": [\r\n            \"generics.host\",\r\n            \"generics.havana\",\r\n            \"generics.ntp\",\r\n            \"mysql.client\",\r\n            \"nova.compute_kvm\",\r\n            \"neutron.openvswitch\"\r\n        ]\r\n    },\r\n    \"config-folder\": \"cluster1\",\r\n</pre>\r\n\r\nAfter this you need to create a directory 'file_root/config/cluster1/goshawk/' and put whatever configurations you want under that.\r\nFor a network node the only file that needs to be there is '/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini'\r\n\r\n\r\nAdding new compute node\r\n=======================\r\n\r\nFollow the below steps to have a new compute node in your cluster\r\n\r\n1. Add a new machine to salt-master.\r\n2. Add it under the list of compute nodes like so\r\n<pre>    \r\n\"compute\": [\r\n         \"lammer\",\r\n     \t \"goshawk\"\r\n    ]\r\n</pre>\r\n3. Under 'file_root/config/cluster1/' add a new directory named 'goshawk'. Copy all the files from 'file_root/config/cluster1/lammer/' into 'file_root/config/cluster1/goshawk'.\r\n4. Run the below command to modify configuration files according to new compute node hostname\r\n<code>\r\nfind file_root/config/cluster1/goshawk -type f -name \\*.\\* -exec sed -i 's/lammer/goshawk/' {} \\;\r\n</code>\r\n5. Finally sync the cluster\r\n<pre>\r\nsalt -C 'I@cluster_type:openstack' state.highstate\r\n</pre>\r\n\r\nThe above methodology can be used to perform auto scaling, which off course is the next project.\r\n\r\nRemoving a node\r\n===============\r\n\r\nChange the file 'pillar_root/top.sls' from \r\n\r\n<pre>\r\nhavana:\r\n  hawk:\r\n    - cluster1\r\n  lammer:\r\n    - cluster1\r\n</pre>\r\nto\r\n<pre>\r\nhavana:\r\n  hawk:\r\n    - cluster1_inverse\r\n  lammer:\r\n    - cluster1_inverse\r\n</pre>\r\n\r\nNow run \r\n<pre>\r\nsalt (machine id here) state.highstate\r\n</pre>\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}